{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dbLiRflPIoPD"
      },
      "outputs": [],
      "source": [
        "# device = 'cpu'\n",
        "device = 'cuda'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_q-UREAdT1WL"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "import torch\n",
        "\n",
        "class MSA(nn.Module):\n",
        "  def __init__(self, input_dim, embed_dim, num_heads):\n",
        "    '''\n",
        "    input_dim: Dimension of input token embeddings\n",
        "    embed_dim: Dimension of internal key, query, and value embeddings\n",
        "    num_heads: Number of self-attention heads\n",
        "    '''\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.input_dim = input_dim\n",
        "    self.embed_dim = embed_dim\n",
        "    self.num_heads = num_heads\n",
        "\n",
        "    self.K_embed = nn.Linear(input_dim, embed_dim, bias=False)\n",
        "    self.Q_embed = nn.Linear(input_dim, embed_dim, bias=False)\n",
        "    self.V_embed = nn.Linear(input_dim, embed_dim, bias=False)\n",
        "    self.out_embed = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''\n",
        "    x: input of shape (batch_size, max_length, input_dim)\n",
        "    return: output of shape (batch_size, max_length, embed_dim)\n",
        "    '''\n",
        "\n",
        "    batch_size, max_length, given_input_dim = x.shape\n",
        "    assert given_input_dim == self.input_dim\n",
        "    assert max_length % self.num_heads == 0\n",
        "\n",
        "    x = x.reshape(batch_size * max_length, -1)\n",
        "    K = self.K_embed(x).reshape(batch_size, max_length, self.embed_dim) # (batch_size, max_length, embed_dim)\n",
        "    Q = self.Q_embed(x).reshape(batch_size, max_length, self.embed_dim) # (batch_size, max_length, embed_dim)\n",
        "    V = self.V_embed(x).reshape(batch_size, max_length, self.embed_dim) # (batch_size, max_length, embed_dim)\n",
        "\n",
        "    # TODO: split each KQV into heads, by reshaping each into (batch_size, max_length, self.num_heads, indiv_dim)\n",
        "    indiv_dim = self.embed_dim // self.num_heads\n",
        "    K = K.reshape(batch_size, max_length, self.num_heads, indiv_dim)\n",
        "    Q = Q.reshape(batch_size, max_length, self.num_heads, indiv_dim)\n",
        "    V = V.reshape(batch_size, max_length, self.num_heads, indiv_dim)\n",
        "\n",
        "    K = K.permute(0, 2, 1, 3) # (batch_size, num_heads, max_length, embed_dim / num_heads)\n",
        "    Q = Q.permute(0, 2, 1, 3) # (batch_size, num_heads, max_length, embed_dim / num_heads)\n",
        "    V = V.permute(0, 2, 1, 3) # (batch_size, num_heads, max_length, embed_dim / num_heads)\n",
        "\n",
        "    K = K.reshape(batch_size * self.num_heads, max_length, indiv_dim)\n",
        "    Q = Q.reshape(batch_size * self.num_heads, max_length, indiv_dim)\n",
        "    V = V.reshape(batch_size * self.num_heads, max_length, indiv_dim)\n",
        "\n",
        "    # transpose and batch matrix multiply\n",
        "    # This is our K transposed so we can do a simple batched matrix multiplication (see torch.bmm for more details and the quick solution)\n",
        "    K_T = K.permute(0, 2, 1)\n",
        "\n",
        "    QK = torch.bmm(Q, K_T)\n",
        "\n",
        "    # calculate weights by dividing everything by the square root of d (self.embed_dim)\n",
        "    weights = QK / (self.embed_dim ** 0.5)\n",
        "    weights =  F.softmax(weights, dim=2)  # (batch_size * num_heads, max_length, max_length)\n",
        "\n",
        "    #  get weighted average... see torch.bmm for a one line solution\n",
        "    # (batch_size * num_heads, max_length, indiv_dim)\n",
        "    # weights is (batch_size * num_heads, max_length, max_length) and V is (batch_size * self.num_heads, max_length, indiv_dim)\n",
        "    # so we want the matrix multiplication of weights and V\n",
        "    w_V = torch.bmm(weights, V)\n",
        "\n",
        "    # rejoin heads\n",
        "    w_V = w_V.reshape(batch_size, self.num_heads, max_length, indiv_dim)\n",
        "    w_V = w_V.permute(0, 2, 1, 3) # (batch_size, max_length, num_heads, embed_dim / num_heads)\n",
        "    w_V = w_V.reshape(batch_size, max_length, self.embed_dim)\n",
        "\n",
        "    out = self.out_embed(w_V)\n",
        "\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoajGRYKsI29"
      },
      "source": [
        "### Implement the ViT architecture\n",
        "You will be implementing the ViT architecture based on the \"An image is worth 16x16 words\" paper.\n",
        "\n",
        "Although the ViT and Transformer architecture are very similar, note a few differences:\n",
        "\n",
        "1. Image patches instead of discrete tokens as input.\n",
        "2. [GELU](https://pytorch.org/docs/stable/generated/torch.nn.GELU.html) is used for the linear layers in the transformer layer (instead of ReLU)\n",
        "3. LayerNorm before the sublayer instead of after.\n",
        "4. Dropout after every linear layer except for KQV projections and also directly after adding positional embeddings to the patch embeddings.\n",
        "5. Learnable [CLS] token at the beginning of the input.\n",
        "\n",
        "A useful reference is Figure 1 in the [paper](https://arxiv.org/pdf/2010.11929.pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Blpaw39U5Y9C"
      },
      "source": [
        "First, implement a single layer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DffgJiJDsA9Y"
      },
      "outputs": [],
      "source": [
        "class ViTLayer(nn.Module):\n",
        "  def __init__(self, num_heads, input_dim, embed_dim, mlp_hidden_dim, dropout=0.1):\n",
        "    '''\n",
        "    num_heads: Number of heads for multi-head self-attention\n",
        "    embed_dim: Dimension of internal key, query, and value embeddings\n",
        "    mlp_hidden_dim: Hidden dimension of the linear layer\n",
        "    dropout: Dropout rate\n",
        "    '''\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.input_dim = input_dim\n",
        "    self.msa = MSA(input_dim, embed_dim, num_heads)\n",
        "\n",
        "    self.layernorm1 = nn.LayerNorm(embed_dim)\n",
        "    self.w_o_dropout = nn.Dropout(dropout)\n",
        "    self.layernorm2 = nn.LayerNorm(embed_dim)\n",
        "    self.mlp = nn.Sequential(nn.Linear(embed_dim, mlp_hidden_dim),\n",
        "                              nn.GELU(),\n",
        "                              nn.Dropout(dropout),\n",
        "                              nn.Linear(mlp_hidden_dim, embed_dim),\n",
        "                              nn.Dropout(dropout))\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''\n",
        "    x: input embeddings (batch_size, max_length, input_dim)\n",
        "    return: output embeddings (batch_size, max_length, embed_dim)\n",
        "    '''\n",
        "\n",
        "    attention_output = self.msa(x)\n",
        "    x += self.w_o_dropout(attention_output)\n",
        "    x = self.layernorm1(x)\n",
        "    linear_output = self.mlp(x)\n",
        "    x += self.w_o_dropout(linear_output)\n",
        "    x = self.layernorm2(x)\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8CONmQMfK4m"
      },
      "source": [
        "A portion of the full network is already implemented for you. Your task is to implement the preprocessing code, converting raw images into patch embeddings + positional embeddings + dropout, with a learnable CLS token at the beginning of the input.\n",
        "\n",
        "Note that patch embeddings are to be added to positional embeddings elementwise, so the input embedding dimensions is size embed_dim."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKbFJj447myz"
      },
      "outputs": [],
      "source": [
        "class ViT(nn.Module):\n",
        "    def __init__(self, patch_dim, image_dim, num_layers, num_heads, embed_dim, mlp_hidden_dim, num_classes, dropout):\n",
        "        '''\n",
        "        patch_dim: patch length and width to split image by\n",
        "        image_dim: image length and width\n",
        "        num_layers: number of layers in network\n",
        "        num_heads: number of heads for multi-head attention\n",
        "        embed_dim: dimension to project images patches to and dimension to use for position embeddings\n",
        "        mlp_hidden_dim: hidden dimension of linear layer\n",
        "        num_classes: number of classes to classify in data\n",
        "        dropout: dropout rate\n",
        "        '''\n",
        "\n",
        "        super().__init__()\n",
        "        self.num_layers = num_layers\n",
        "        self.patch_dim = patch_dim\n",
        "        self.image_dim = image_dim\n",
        "        self.input_dim = self.patch_dim * self.patch_dim * 3\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        self.patch_embedding = nn.Linear(self.input_dim, embed_dim)\n",
        "        self.position_embedding = nn.Parameter(torch.zeros(1, (image_dim // patch_dim) ** 2 + 1, embed_dim))\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.embedding_dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList([])\n",
        "        for i in range(num_layers):\n",
        "            self.encoder_layers.append(ViTLayer(num_heads, embed_dim, embed_dim, mlp_hidden_dim, dropout))\n",
        "\n",
        "        self.mlp_head = nn.Linear(embed_dim, num_classes)\n",
        "        self.layernorm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, images):\n",
        "        '''\n",
        "        images: raw image data (batch_size, channels, rows, cols)\n",
        "        '''\n",
        "\n",
        "        # Don't hardcode dimensions (except for maybe channels = 3), use the variables in __init__.\n",
        "        # You shouldn't need to add anything else to __init__, all of the embeddings,\n",
        "        # dropout etc. are already initialized for you.\n",
        "\n",
        "        # Put the preprocessed patches in variable \"out\" with shape (batch_size, length, embed_dim).\n",
        "\n",
        "        h = w = self.image_dim // self.patch_dim\n",
        "        N = images.size(0)\n",
        "        images = images.reshape(N, 3, h, self.patch_dim, w, self.patch_dim)\n",
        "        images = torch.einsum(\"nchpwq -> nhwpqc\", images)\n",
        "        patches = images.reshape(N, h * w, self.input_dim) # (batch, num_patches_per_image, patch_size_unrolled)\n",
        "\n",
        "        patch_embeddings = self.patch_embedding(patches)\n",
        "        patch_embeddings = torch.cat([torch.tile(self.cls_token, (N, 1, 1)), patch_embeddings], dim=1)\n",
        "        out = patch_embeddings + torch.tile(self.position_embedding, (N, 1, 1)) # We add positional embeddings to our tokens (not concatenated)\n",
        "        out = self.embedding_dropout(out)\n",
        "\n",
        "        # add padding s.t. input length is multiple of num_heads\n",
        "        add_len = (self.num_heads - out.shape[1]) % self.num_heads\n",
        "        out = torch.cat([out, torch.zeros(N, add_len, out.shape[2], device=device)], dim=1)\n",
        "\n",
        "        # Pass through each one of our encoder layers\n",
        "        for layer in self.encoder_layers:\n",
        "            out = layer(out)\n",
        "\n",
        "        # Pop off and read our classification token we added, see what the value is\n",
        "        cls_head = self.layernorm(torch.squeeze(out[:, 0], dim=1))\n",
        "        logits = self.mlp_head(cls_head)\n",
        "        return logits\n",
        "\n",
        "def get_vit_tiny(num_classes=10, patch_dim=4, image_dim=32):\n",
        "    return ViT(patch_dim=patch_dim, image_dim=image_dim, num_layers=12, num_heads=3,\n",
        "               embed_dim=192, mlp_hidden_dim=768, num_classes=num_classes, dropout=0.1)\n",
        "\n",
        "def get_vit_small(num_classes=10, patch_dim=4, image_dim=32):\n",
        "    return ViT(patch_dim=patch_dim, image_dim=image_dim, num_layers=12, num_heads=6,\n",
        "               embed_dim=384, mlp_hidden_dim=1536, num_classes=num_classes, dropout=0.1)\n",
        "\n",
        "def get_vit_base(num_classes=10, patch_dim=4, image_dim=32):\n",
        "    return ViT(patch_dim=patch_dim, image_dim=image_dim, num_layers=12, num_heads=12,\n",
        "               embed_dim=768, mlp_hidden_dim=3072, num_classes=num_classes, dropout=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fkN5vhqp9eI7",
        "outputId": "2f3144be-0dbc-4a6b-98dd-7b1a1e764c9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar10/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 170498071/170498071 [00:04<00:00, 37289802.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar10/cifar-10-python.tar.gz to ./data/cifar10\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision.transforms as T\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision\n",
        "import math\n",
        "import torch.optim as optim\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "data_root = './data/cifar10'\n",
        "train_size = 40000\n",
        "val_size = 10000\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "transform_train = T.Compose([\n",
        "    T.Resize(40),\n",
        "    T.RandomCrop(32),\n",
        "    T.RandomHorizontalFlip(),\n",
        "    T.RandomAffine(degrees=0, translate=(0.2, 0.2), scale=(0.95, 1.05)),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "transform_val = T.Compose([\n",
        "    T.ToTensor(),\n",
        "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(\n",
        "    root=data_root,\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform_train,\n",
        ")\n",
        "\n",
        "val_dataset = datasets.CIFAR10(\n",
        "    root=data_root,\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=transform_val,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450,
          "referenced_widgets": [
            "6cfb2f7d8eda4b9c9d3bd23bfe21c80f",
            "635cc9dd76fd4e6184ff94c30c69c50a",
            "172a3a9b4cb04dc6b0a02a1b00c5bbe4",
            "c11c9178120644e2b5dc52fedc5f5752",
            "0dbe660616d943908142581e216920ae",
            "a7a36a8358034bc59e536de663e31e48",
            "3c2e77b6034c46ac812c7b45781b71ab",
            "e13ba3e3b3df460886a9607da05078b3",
            "d473c682212843feaeae09d5fc21412e",
            "4c23ba5e376a41c58f6a9fc0b5d842e8",
            "988f6afa78734abf94f361b34b476b25"
          ]
        },
        "id": "73B3f3nLTjWM",
        "outputId": "d654eafa-3f72-44eb-fa79-3d9c8aefe3b7"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import sampler\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size,\n",
        "                          sampler=sampler.SubsetRandomSampler(range(train_size)))\n",
        "\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size,\n",
        "                        sampler=sampler.SubsetRandomSampler(range(train_size, 50000)))\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "vit = get_vit_small().to(device)\n",
        "\n",
        "learning_rate = 5e-4 * batch_size / 256\n",
        "num_epochs = 30\n",
        "weight_decay = 0.1\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(vit.parameters(), lr=learning_rate, betas=(0.9, 0.95), weight_decay=weight_decay)\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss = 0.0\n",
        "    train_acc = 0.0\n",
        "    train_total = 0\n",
        "    vit.train()\n",
        "    for inputs, labels in tqdm(train_loader):\n",
        "        \"\"\"TODO:\n",
        "        1. Set inputs and labels to be on device\n",
        "        2. zero out our gradients\n",
        "        3. pass our inputs through the ViT\n",
        "        4. pass our outputs / labels into our loss / criterion\n",
        "        5. backpropagate\n",
        "        6. step our optimizeer\n",
        "        \"\"\"\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = vit(inputs)\n",
        "\n",
        "        loss = criterion(outputs, labels.long())\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * inputs.shape[0]\n",
        "        train_acc += torch.sum((torch.argmax(outputs, dim=1) == labels)).item()\n",
        "        train_total += inputs.shape[0]\n",
        "    train_loss = train_loss / train_total\n",
        "    train_acc = train_acc / train_total\n",
        "    train_losses.append(train_loss)\n",
        "\n",
        "    val_loss = 0.0\n",
        "    val_acc = 0.0\n",
        "    val_total = 0\n",
        "    vit.eval()\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = vit(inputs)\n",
        "            loss = criterion(outputs, labels.long())\n",
        "\n",
        "            val_loss += loss.item() * inputs.shape[0]\n",
        "            val_acc += torch.sum((torch.argmax(outputs, dim=1) == labels)).item()\n",
        "            val_total += inputs.shape[0]\n",
        "    val_loss = val_loss / val_total\n",
        "    val_acc = val_acc / val_total\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    val_accuracies.append(val_acc)\n",
        "    if val_acc >= max(val_accuracies):\n",
        "        torch.save(vit.state_dict(), 'best_model.pth')\n",
        "\n",
        "    print(f'[{epoch + 1:2d}] train loss: {train_loss:.3f} | train accuracy: {train_acc:.3f} | val loss: {val_loss:.3f} | val accuracy: {val_acc:.3f}')\n",
        "\n",
        "print('Finished Training')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0dbe660616d943908142581e216920ae": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "172a3a9b4cb04dc6b0a02a1b00c5bbe4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e13ba3e3b3df460886a9607da05078b3",
            "max": 1250,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d473c682212843feaeae09d5fc21412e",
            "value": 21
          }
        },
        "3c2e77b6034c46ac812c7b45781b71ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4c23ba5e376a41c58f6a9fc0b5d842e8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "635cc9dd76fd4e6184ff94c30c69c50a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a7a36a8358034bc59e536de663e31e48",
            "placeholder": "​",
            "style": "IPY_MODEL_3c2e77b6034c46ac812c7b45781b71ab",
            "value": "  2%"
          }
        },
        "6cfb2f7d8eda4b9c9d3bd23bfe21c80f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_635cc9dd76fd4e6184ff94c30c69c50a",
              "IPY_MODEL_172a3a9b4cb04dc6b0a02a1b00c5bbe4",
              "IPY_MODEL_c11c9178120644e2b5dc52fedc5f5752"
            ],
            "layout": "IPY_MODEL_0dbe660616d943908142581e216920ae"
          }
        },
        "988f6afa78734abf94f361b34b476b25": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a7a36a8358034bc59e536de663e31e48": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c11c9178120644e2b5dc52fedc5f5752": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c23ba5e376a41c58f6a9fc0b5d842e8",
            "placeholder": "​",
            "style": "IPY_MODEL_988f6afa78734abf94f361b34b476b25",
            "value": " 21/1250 [00:01&lt;01:06, 18.54it/s]"
          }
        },
        "d473c682212843feaeae09d5fc21412e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e13ba3e3b3df460886a9607da05078b3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
